{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca3c370",
   "metadata": {},
   "source": [
    "# Exercise 3 ¬∑ Missing-Data Case Studies  \n",
    "*Notebook Overview & Road-Map*\n",
    "\n",
    "Welcome to **Exercise 3**, a two-part, eight-case sprint through real-world missing-data puzzles inspired by the ABCD Study.  \n",
    "You‚Äôll practice *detecting*, *classifying*, and *cleaning* a variety of missingness patterns‚Äîthen see how each choice changes your results.\n",
    "\n",
    "## üéØ Overall Learning Objectives\n",
    "\n",
    "1. **Identify & classify structured missing-value codes**  \n",
    "   Locate `777 Refused`, `999 Don‚Äôt-know`, sentinel blanks, and branch-skip cells and explain what each signals.\n",
    "\n",
    "2. **Differentiate MCAR / MAR / MNAR scenarios**  \n",
    "   Use skip patterns, non-response codes, and quick EDA to defend a mechanism diagnosis.\n",
    "\n",
    "3. **Select & justify handling strategies**  \n",
    "   Recommend (and code) a simple but appropriate remedy then quantify how a na√Øve vs. cleaned approach shifts key statistics.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Part 1 ( Case 1 ‚Äì 4 ) ‚Äî **Mechanisms in Miniature**  \n",
    "Four bite-sized toy surveys walk you through the **three canonical mechanisms**:\n",
    "\n",
    "| Case | Scenario (1-sentence) | Likely Mechanism | Skill Focus |\n",
    "|------|-----------------------|------------------|-------------|\n",
    "| 1 | A single row randomly corrupted | **MCAR** | Harmless deletion vs. mean-fill |\n",
    "| 2 | High-ADHD teens quit late items | **MAR** | Imputation choices & bias |\n",
    "| 3 | High-risk youth drop 6-mo follow-up | **MNAR (or MAR)** | Conservative fixes & sensitivity |\n",
    "| 4 | High-stress students skip depression item | **MAR ‚Üí regression** | Using strong predictors to impute |\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Part 2 ( Case 5 ‚Äì 8 ) ‚Äî **ABCD-Style Cases**\n",
    "\n",
    "| Case | Instrument & Twist | Special Codes to Untangle |\n",
    "|------|-------------------|---------------------------|\n",
    "| 5 | **Youth Cigarette Expectancies** ‚Äî item added mid-wave | Wave-design blanks + `777` Decline |\n",
    "| 6 | **Parental Rules (Alcohol & Vaping)** ‚Äî new ENDS branch | Wave blanks ¬∑ branch blanks ¬∑ `777` |\n",
    "| 7 | **Household Marijuana-Smoke Exposure** ‚Äî numeric follow-ups | Blank vs DK-flag `999` |\n",
    "| 8 | **School Attendance & Grades** ‚Äî IEP branch logic | `444` Not-Applicable ¬∑ `777` Decline |\n",
    "\n",
    "Each Part 2 notebook follows an identical 5-step path: **Setup ‚ñ∏ Codebook Audit ‚ñ∏ Visual Diagnostics ‚ñ∏ Na√Øve vs Clean Bias Check ‚ñ∏ Reflection**.\n",
    "\n",
    "### Folder Structure  \n",
    "Download the **Exercise3.zip** from Canvas; you‚Äôll find:  \n",
    "- `Exercise3.ipynb` ‚Äì The working notebook with Parts 1 & 2  \n",
    "-  synthetic_su_y_cigexp_sample.csv # Case 5\n",
    "-  synthetic_su_p_rule_sample.csv # Case 6\n",
    "-  synthetic_mj_smoke_sample.csv # Case 7\n",
    "-  synthetic_fc_p_sag_sample.csv # Case 8\n",
    "\n",
    "### Collaboration & Submission  \n",
    "1. **Group Work:**  \n",
    "   - Open the shared **Exercise 3 PowerPoint** in Canvas Collaborations  \n",
    "   - As a team, complete the slides corresponding to each Part 1 case and Part 2 hunt  \n",
    "2. **Individual Submission:**  \n",
    "   - Once your group has finalized the shared PPT, **download your own copy**  \n",
    "   - Submit that PowerPoint under **Canvas ‚Ä∫ Assignments ‚Ä∫ Exercise 3**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b606356",
   "metadata": {},
   "source": [
    "## Part 1 ‚Äì Missing Mechanism Case Studies  \n",
    "In the first half of **E3** you‚Äôll work through four bite-sized, self-contained surveys that illustrate the three canonical missing-data mechanisms:\n",
    "\n",
    "| Case | Nick-name & Scenario | Likely Mechanism | Key Learning Goal |\n",
    "|------|----------------------|------------------|-------------------|\n",
    "| 1 | **‚ÄúThe Lost Survey‚Äù** ‚Äì one entire respondent‚Äôs row is corrupted | MCAR | Spot purely random loss and gauge harmless remedies |\n",
    "| 2 | **‚ÄúSurvey Fatigue Study‚Äù** ‚Äì high-ADHD teens skip the last 3 items | MAR | Trace missingness back to an observed trait and compare imputation styles |\n",
    "| 3 | **‚ÄúTreatment Drop-Out Study‚Äù** ‚Äì high-risk youth disappear at follow-up | MNAR (or MAR) | Debate whether relapse-related non-response is ignorable and test conservative fixes |\n",
    "| 4 | **‚ÄúMental-Health Screening‚Äù** ‚Äì high-stress students skip the depression item | MAR ‚Üí regression | Exploit strong predictors to impute a sensitive outcome |\n",
    "\n",
    "For each case you will:\n",
    "\n",
    "1. **Diagnose** the mechanism using quick EDA.  \n",
    "2. **Choose** one ad-hoc remedy and apply it.  \n",
    "3. **Reflect** on how the remedy alters sample size, means, and variances.\n",
    "\n",
    "### Team Assignment Table\n",
    "\n",
    "| Group Member | Lead Case Study |\n",
    "|--------------|-----------------|\n",
    "| **Member 1** | Case 1 ‚Äì The Lost Survey (MCAR) |\n",
    "| **Member 2** | Case 2 ‚Äì Survey Fatigue (MAR) |\n",
    "| **Member 3** | Case 3 ‚Äì Treatment Drop-Out (MNAR/MAR) |\n",
    "| **Member 4** | Case 4 ‚Äì Mental-Health Screen (MAR ‚Üí Reg Impute) |\n",
    "\n",
    "*If your group has three people then simply skip a case study.*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b374872",
   "metadata": {},
   "source": [
    "## Case 1 ‚Äì ‚ÄúThe Lost Survey‚Äù  *(MCAR Candidate)*  \n",
    "A REDCap export glitch corrupted one participant‚Äôs questionnaire file.  \n",
    "Only the *fourth* row of data is affected; all other rows are intact.  \n",
    "Your job:\n",
    "\n",
    "1. **Explore** the pattern of missing values.  \n",
    "2. **Diagnose** the likely missingness mechanism.  \n",
    "3. **Apply** one simple remedy and comment on its impact.\n",
    "\n",
    "> *Hint:* The glitch was random‚Äîany participant‚Äôs row could have been lost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98641375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1: set-up & quick EDA\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "case1 = pd.DataFrame({\n",
    "    \"q_anxiety\":[1, 2, 3, np.nan],\n",
    "    \"q_stress\" :[2, 2, 4, np.nan],\n",
    "    \"q_sleep\"  :[7, 6, 5, np.nan]\n",
    "})\n",
    "\n",
    "display(case1)\n",
    "\n",
    "# basic summary\n",
    "print(\"\\nMissing count by column:\")\n",
    "print(case1.isna().sum())\n",
    "\n",
    "print(\"\\nMissing count by row:\")\n",
    "print(case1.isna().sum(axis=1))\n",
    "\n",
    "# Visualize missing data pattern\n",
    "print(\"\\nMissing Data Pattern:\")\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(case1.isna(), \n",
    "            yticklabels=[\"Row 1\", \"Row 2\", \"Row 3\", \"Row 4\"],\n",
    "            cbar_kws={'label': 'Missing Data'},\n",
    "            cmap=['lightblue', 'red'])\n",
    "plt.title(\"Missing Values Heatmap\\n(Red = Missing, Blue = Present)\")\n",
    "plt.xlabel(\"Survey Questions\")\n",
    "plt.ylabel(\"Participants\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Alternative simple visualization\n",
    "print(\"\\nSimple missing pattern check:\")\n",
    "missing_pattern = case1.isna().astype(int)\n",
    "print(missing_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b620b8",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Diagnose the Missingness Mechanism  \n",
    "*Use the space below to jot down your reasoning.*\n",
    "\n",
    "- **Observed pattern:** Only row 4 is missing *every* value.  \n",
    "- **Likely mechanism (MCAR / MAR / MNAR):**  \n",
    "- **Why?**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08ada3",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Choose & Apply a Remedy  \n",
    "Pick **one** of the three ad-hoc strategies:\n",
    "\n",
    "| Option | When It Makes Sense | One-liner Hint |\n",
    "|--------|---------------------|----------------|\n",
    "| **A. Listwise Deletion** | MCAR & small data loss acceptable | `clean = case1.dropna()` |\n",
    "| **B. LOCF** | Longitudinal data missing a time-point | `clean = case1.ffill()` |\n",
    "| **C. Mean / Median Imputation** | Small % missing, symmetric distrib. | `clean = case1.fillna(case1.mean())` |\n",
    "\n",
    "> **Task:** Choose the approach that best fits your diagnosed mechanism and evaluate its assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Remedy code cell ---\n",
    "# ‚¨áÔ∏è Uncomment the approach you decide to test.\n",
    "\n",
    "# ----- Option A: Listwise Deletion -----\n",
    "# clean = case1.dropna()\n",
    "\n",
    "# ----- Option B: LOCF (not ideal here) -----\n",
    "# clean = case1.ffill()\n",
    "\n",
    "# ----- Option C: Mean Imputation -----\n",
    "# clean = case1.fillna(case1.mean())\n",
    "\n",
    "print(\"Cleaned data:\")\n",
    "display(clean)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä BEFORE vs AFTER COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"BEFORE cleaning:\")\n",
    "print(f\"Sample size: {len(case1)} participants\")\n",
    "print(f\"Complete responses: {case1.dropna().shape[0]} participants\")\n",
    "print(f\"Missing values: {case1.isna().sum().sum()} total\")\n",
    "\n",
    "print(\"\\nBEFORE - Descriptive statistics (complete cases only):\")\n",
    "print(case1.describe().round(2))\n",
    "\n",
    "print(\"\\n\" + \"-\"*30)\n",
    "print(\"AFTER cleaning:\")\n",
    "print(f\"Sample size: {len(clean)} participants\")\n",
    "print(f\"Complete responses: {clean.dropna().shape[0]} participants\") \n",
    "print(f\"Missing values: {clean.isna().sum().sum()} total\")\n",
    "\n",
    "print(\"\\nAFTER - Descriptive statistics:\")\n",
    "print(clean.describe().round(2))\n",
    "\n",
    "print(\"\\n\" + \"-\"*30)\n",
    "print(\"üîç KEY CHANGES:\")\n",
    "print(f\"Rows lost/gained: {len(clean) - len(case1.dropna())}\")\n",
    "print(f\"Values added: {clean.count().sum() - case1.count().sum()}\")\n",
    "\n",
    "# Compare means for each variable\n",
    "print(\"\\nMean comparison:\")\n",
    "for col in case1.columns:\n",
    "    before_mean = case1[col].mean()\n",
    "    after_mean = clean[col].mean() \n",
    "    change = after_mean - before_mean\n",
    "    print(f\"{col:12}: {before_mean:5.2f} ‚Üí {after_mean:5.2f} (Œî={change:+5.2f})\")\n",
    "\n",
    "# Compare standard deviations  \n",
    "print(\"\\nStandard deviation comparison:\")\n",
    "for col in case1.columns:\n",
    "    before_std = case1[col].std()\n",
    "    after_std = clean[col].std()\n",
    "    change = after_std - before_std if pd.notna(before_std) else np.nan\n",
    "    print(f\"{col:12}: {before_std:5.2f} ‚Üí {after_std:5.2f} (Œî={change:+5.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04a0435",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Reflection ‚Äì How Did the Remedy Change Your Data?  \n",
    "#### **Answer Reflection Questions in Working Table 1 in the Group Document**\n",
    "\n",
    "**Use the comparison output above to answer:**\n",
    "\n",
    "- **Method used:** [A/B/C]\n",
    "- **Rows/values removed or altered:** *(Look at \"Sample size\" and \"Values added\" from output)*\n",
    "- **Mean & variance differences:** *(Check the \"Mean comparison\" and \"Standard deviation comparison\" sections)*  \n",
    "- **Would your substantive conclusions change?** *(Consider: Are the changes large enough to matter for your research question?)*\n",
    "\n",
    "**Write 3-4 sentences summarizing:**\n",
    "1. What specific changes occurred to your dataset?\n",
    "2. Whether these changes would affect conclusions about anxiety, stress, and sleep relationships?\n",
    "3. Why your chosen method was/wasn't appropriate for this scenario?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe976b",
   "metadata": {},
   "source": [
    "## Case 2 ‚Äì \"The Survey Fatigue Study\"  *(MAR Candidate)*  \n",
    "A research team is studying adolescent attitudes toward substance use using a comprehensive 10-question survey. During data collection, they notice a concerning pattern: participants with higher **ADHD symptoms** (measured on the DSM-aligned 0-18 scale) tend to have incomplete responses, with systematic drop-off occurring in the **later questions** of the survey.\n",
    "\n",
    "The research assistant hypothesizes that attention difficulties make it harder for some teens to complete longer questionnaires, leading to systematic missingness that depends on an **observed characteristic** rather than the missing answers themselves.\n",
    "\n",
    "**Your Investigation:**\n",
    "\n",
    "1. **Explore** the missing data pattern across ADHD severity levels  \n",
    "2. **Diagnose** the likely missingness mechanism (MCAR / MAR / MNAR)  \n",
    "3. **Apply** a remedy and evaluate its impact on the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848faf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2: set-up & quick EDA - Extended 10-question survey\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 12 participants with varying ADHD severity and progressive survey fatigue\n",
    "case2 = pd.DataFrame({\n",
    "    \"participant_id\": [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112],\n",
    "    \"adhd_score\":     [ 3,   7,  10,  12,  14,  15,  16,  17,  17,  18,   5,   9],  # DSM 0-18 scale\n",
    "    \n",
    "    # Questions 1-7: Everyone completes these (regardless of ADHD)\n",
    "    \"attitude_q1\":    [ 2,   3,   4,   3,   5,   2,   4,   5,   3,   4,   2,   3],\n",
    "    \"attitude_q2\":    [ 3,   2,   3,   4,   4,   3,   5,   4,   2,   5,   3,   2],\n",
    "    \"attitude_q3\":    [ 1,   2,   2,   3,   3,   4,   3,   2,   4,   3,   2,   1],\n",
    "    \"attitude_q4\":    [ 2,   1,   3,   2,   4,   3,   4,   3,   2,   4,   1,   2],\n",
    "    \"attitude_q5\":    [ 3,   2,   4,   3,   2,   4,   3,   4,   2,   3,   3,   2],\n",
    "    \"attitude_q6\":    [ 2,   3,   2,   4,   3,   2,   2,   3,   4,   2,   2,   3],\n",
    "    \"attitude_q7\":    [ 4,   2,   3,   2,   4,   1,   3,   2,   1,   4,   3,   2],\n",
    "    \n",
    "    # Later questions (Q8-Q10): Only ADHD ‚â•16 participants drop out\n",
    "    \"attitude_q8\":    [ 1,   3,   2,   3,   4,   2, np.nan, np.nan, np.nan, np.nan, 2, 3],\n",
    "    \"attitude_q9\":    [ 3,   2,   1,   2,   3,   4, np.nan, np.nan, np.nan, np.nan, 1, 2],\n",
    "    \"attitude_q10\":   [ 2,   4,   3,   1,   2,   3,   2, np.nan, np.nan, np.nan, 3, 1]\n",
    "})\n",
    "\n",
    "display(case2)\n",
    "\n",
    "print(\"\\nMissing count by column:\")\n",
    "print(case2.isna().sum())\n",
    "\n",
    "print(\"\\nMissing count by participant:\")\n",
    "attitude_cols = [f\"attitude_q{i}\" for i in range(1, 11)]\n",
    "missing_by_participant = case2[attitude_cols].isna().sum(axis=1)\n",
    "print(missing_by_participant)\n",
    "\n",
    "# Create comprehensive analysis\n",
    "case2['total_missing'] = case2[attitude_cols].isna().sum(axis=1)\n",
    "case2['completion_rate'] = (10 - case2['total_missing']) / 10 * 100\n",
    "\n",
    "print(\"\\nCompletion Analysis:\")\n",
    "completion_summary = case2[['participant_id', 'adhd_score', 'total_missing', 'completion_rate']].copy()\n",
    "print(completion_summary)\n",
    "\n",
    "# Visualization\n",
    "print(\"\\nMissing Data Pattern Analysis:\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(case2[\"adhd_score\"], case2[\"total_missing\"], alpha=0.7, s=80, c='steelblue')\n",
    "plt.xlabel(\"ADHD Score (0-18 scale)\")\n",
    "plt.ylabel(\"Number of Missing Items\")\n",
    "plt.title(\"ADHD Severity vs. Missing Items\")\n",
    "plt.axvline(x=16, color='red', linestyle='--', alpha=0.7, label='Clinical threshold (‚â•16)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nADHD Score Analysis:\")\n",
    "complete_cases = case2[case2['total_missing']==0]\n",
    "incomplete_cases = case2[case2['total_missing']>0]\n",
    "\n",
    "print(f\"Mean ADHD score - Complete cases (0 missing): {complete_cases['adhd_score'].mean():.1f}\")\n",
    "print(f\"Mean ADHD score - Incomplete cases (1+ missing): {incomplete_cases['adhd_score'].mean():.1f}\")\n",
    "print(f\"\\nSeverity breakdown:\")\n",
    "print(f\"Participants with ADHD ‚â•16: {len(case2[case2['adhd_score']>=16])}\")\n",
    "print(f\"Mean completion rate for ADHD ‚â•16: {case2[case2['adhd_score']>=16]['completion_rate'].mean():.1f}%\")\n",
    "print(f\"Mean completion rate for ADHD <16: {case2[case2['adhd_score']<16]['completion_rate'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3463d",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Diagnose the Missingness Mechanism  \n",
    "*Write your reasoning below.*\n",
    "\n",
    "- **Observed pattern:** Only participants with **higher ADHD scores** drop questions near the end.  \n",
    "- **Likely mechanism:**  \n",
    "- **Justification (why MAR fits better than MCAR or MNAR):**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b3ec84",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Listwise Deletion Analysis ‚Äì What Do We Lose?\n",
    "Before applying imputation, let's first see what happens with **listwise deletion** - the \"default\" approach that simply removes incomplete cases.\n",
    "\n",
    "**Key Questions:**\n",
    "- Which participants get excluded?\n",
    "- How does this change our sample characteristics?\n",
    "- What bias might this introduce for MAR data?\n",
    "\n",
    "> This will help us understand **why** imputation might be necessary for this MAR scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2996a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Listwise Deletion Analysis ---\n",
    "attitude_cols = [f\"attitude_q{i}\" for i in range(1, 11)]\n",
    "complete_cases = case2.dropna(subset=attitude_cols)\n",
    "\n",
    "print(f\"Original sample: {len(case2)} participants\")\n",
    "print(f\"Complete cases: {len(complete_cases)} participants\") \n",
    "print(f\"Lost: {len(case2) - len(complete_cases)} participants\")\n",
    "\n",
    "print(f\"\\nWho gets excluded:\")\n",
    "excluded_participants = case2[~case2.index.isin(complete_cases.index)]\n",
    "print(excluded_participants[['participant_id', 'adhd_score', 'total_missing']])\n",
    "\n",
    "print(f\"\\nMean ADHD score - Original: {case2['adhd_score'].mean():.1f}\")\n",
    "print(f\"Mean ADHD score - Complete cases: {complete_cases['adhd_score'].mean():.1f}\")\n",
    "print(f\"High-ADHD (‚â•16) lost: {len(case2[case2['adhd_score']>=16]) - len(complete_cases[complete_cases['adhd_score']>=16])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6240ea",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Imputation Comparison ‚Äì Preserve Individual Patterns?\n",
    "Now let's compare **two imputation approaches** to see which better handles this MAR scenario:\n",
    "\n",
    "| Option | Approach | Best For |\n",
    "|--------|----------|----------|\n",
    "| **A. Sample Mean Imputation** | Use population averages | Assumes missing participants are \"typical\" |\n",
    "| **B. Participant-Specific Mean**  | Use each person's own pattern | Preserves individual differences |\n",
    "\n",
    "**Task:** Try both approaches and compare how they handle high-ADHD participants differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e1ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Compare Imputation Approaches ---\n",
    "print(\"üîÑ IMPUTATION COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ----- Option A: Sample Mean Imputation -----\n",
    "clean_sample = case2.fillna(case2.mean(numeric_only=True))\n",
    "\n",
    "# ----- Option B: Participant-Specific Mean Imputation -----\n",
    "clean_individual = case2.copy()\n",
    "attitude_cols = [f\"attitude_q{i}\" for i in range(1, 11)]\n",
    "\n",
    "for idx, row in clean_individual.iterrows():\n",
    "    participant_responses = row[attitude_cols]\n",
    "    participant_mean = participant_responses.mean()  # excludes NaN automatically\n",
    "    clean_individual.loc[idx, attitude_cols] = participant_responses.fillna(participant_mean)\n",
    "\n",
    "# Compare the two approaches for high-ADHD participant (ID 110)\n",
    "id110_original = case2[case2[\"participant_id\"]==110][attitude_cols].iloc[0]\n",
    "id110_sample = clean_sample[clean_sample[\"participant_id\"]==110][attitude_cols].iloc[0]\n",
    "id110_individual = clean_individual[clean_individual[\"participant_id\"]==110][attitude_cols].iloc[0]\n",
    "\n",
    "print(f\"\\nüìä Example: ID 110 (ADHD=18)\")\n",
    "print(f\"Personal mean from completed questions: {id110_original.mean():.2f}\")\n",
    "print(f\"Sample approach - Imputed values: {id110_sample[['attitude_q8', 'attitude_q9', 'attitude_q10']].values}\")\n",
    "print(f\"Individual approach - Imputed values: {id110_individual[['attitude_q8', 'attitude_q9', 'attitude_q10']].values}\")\n",
    "\n",
    "print(f\"\\nüìà Overall Question Means After Imputation:\")\n",
    "print(\"Sample Mean Approach:\")\n",
    "print(clean_sample[attitude_cols].mean().round(2))\n",
    "print(\"\\nParticipant-Specific Approach:\")\n",
    "print(clean_individual[attitude_cols].mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fe235f",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Reflection Questions\n",
    "#### **Answer Reflection Questions in Working Table 1 in the Group Document**\n",
    "\n",
    "**Answer based on your chosen method:**\n",
    "\n",
    "#### If you chose **Participant-Specific Mean Imputation**:\n",
    "- How do the imputed values differ from sample means?\n",
    "- Does this better preserve individual response patterns?\n",
    "- What happens when a participant has very few completed questions?\n",
    "\n",
    "#### If you chose **Sample Mean Imputation**:\n",
    "- Do all high-ADHD participants get identical imputed values?\n",
    "- Does this ignore their individual response patterns?\n",
    "- How might this bias correlations with ADHD scores?\n",
    "\n",
    "**Write 3-4 sentences comparing participant-specific vs. sample-wide imputation for MAR data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50792a0e",
   "metadata": {},
   "source": [
    "## Case 3 ‚Äì \"The Treatment Drop-Out Study\"  \n",
    "A substance abuse treatment program tracks adolescent progress over 6 months using self-report questionnaires about **current alcohol use** and **treatment motivation**. \n",
    "\n",
    "**The Pattern:** During data collection, researchers notice that participants who started with **higher baseline alcohol use** and **lower treatment motivation** systematically fail to complete follow-up surveys. These participants become non-responsive and cannot be reached for their 6-month assessments.\n",
    "\n",
    "**The Dilemma:** Is this missing data due to practical issues (harder to contact high-risk teens) or because participants who relapsed don't want to report their current drinking levels?\n",
    "\n",
    "**Your Investigation:**\n",
    "1. **Explore** the pattern of baseline characteristics vs. follow-up completion\n",
    "2. **Diagnose** the likely missingness mechanism (MCAR / MAR / MNAR)  \n",
    "3. **Apply** a remedy and evaluate its impact on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a60a248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 3: Treatment study - Missing data pattern\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "case3 = pd.DataFrame({\n",
    "    \"participant_id\": [101, 101, 102, 102, 103, 103, 201, 201, 202, 202, 203, 203],\n",
    "    \"timepoint\":      [\"baseline\", \"6-month\", \"baseline\", \"6-month\", \"baseline\", \"6-month\", \n",
    "                       \"baseline\", \"6-month\", \"baseline\", \"6-month\", \"baseline\", \"6-month\"],\n",
    "    \n",
    "    # Baseline risk factors predict who will drop out\n",
    "    \"alcohol_days\":   [15, np.nan, 12, np.nan, 10, np.nan,  # High-risk participants missing\n",
    "                        3,       2,  5,       3,  2,       1], # Low-risk participants continue\n",
    "    \"motivation\":     [ 2, np.nan,  3, np.nan,  3, np.nan,  # High baseline use ‚Üí dropout\n",
    "                        8,       9,  7,       8,  9,       9]  # High motivation ‚Üí continue\n",
    "})\n",
    "\n",
    "# Ensure proper ordering\n",
    "case3 = case3.sort_values([\"participant_id\", \"timepoint\"]).reset_index(drop=True)\n",
    "display(case3)\n",
    "\n",
    "print(\"\\nBaseline Risk vs. Follow-up Completion:\")\n",
    "baseline = case3[case3[\"timepoint\"] == \"baseline\"].reset_index(drop=True)\n",
    "followup_counts = case3.groupby(\"participant_id\")[\"alcohol_days\"].count()\n",
    "\n",
    "# Create completion analysis\n",
    "completion_analysis = pd.DataFrame({\n",
    "    \"participant_id\": baseline[\"participant_id\"],\n",
    "    \"baseline_alcohol_days\": baseline[\"alcohol_days\"],\n",
    "    \"baseline_motivation\": baseline[\"motivation\"],\n",
    "    \"completed_followup\": [followup_counts[pid] > 1 for pid in baseline[\"participant_id\"]]\n",
    "})\n",
    "print(completion_analysis)\n",
    "\n",
    "# Pattern analysis\n",
    "completers = completion_analysis[completion_analysis[\"completed_followup\"]]\n",
    "dropouts = completion_analysis[~completion_analysis[\"completed_followup\"]]\n",
    "\n",
    "print(f\"\\nPattern Evidence:\")\n",
    "print(f\"Mean baseline alcohol days - Completers: {completers['baseline_alcohol_days'].mean():.1f}\")\n",
    "print(f\"Mean baseline alcohol days - Drop-outs: {dropouts['baseline_alcohol_days'].mean():.1f}\")\n",
    "print(f\"Mean baseline motivation - Completers: {completers['baseline_motivation'].mean():.1f}\")\n",
    "print(f\"Mean baseline motivation - Drop-outs: {dropouts['baseline_motivation'].mean():.1f}\")\n",
    "\n",
    "# Box plot visualization\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä VISUAL ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "completion_data = [\n",
    "    completers['baseline_alcohol_days'].values,\n",
    "    dropouts['baseline_alcohol_days'].values\n",
    "]\n",
    "box_plot = plt.boxplot(completion_data, labels=['Completed', 'Dropped Out'], patch_artist=True)\n",
    "box_plot['boxes'][0].set_facecolor('lightgreen')\n",
    "box_plot['boxes'][1].set_facecolor('lightcoral')\n",
    "plt.ylabel('Baseline Alcohol Days/Month')\n",
    "plt.title('Alcohol Use Distribution by Follow-up Completion Status')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics table\n",
    "print(f\"\\nüìà COMPLETION SUMMARY TABLE:\")\n",
    "print(\"-\" * 40)\n",
    "summary_table = pd.DataFrame({\n",
    "    'Group': ['Completers', 'Drop-outs', 'Difference'],\n",
    "    'N': [len(completers), len(dropouts), ''],\n",
    "    'Mean Alcohol Days': [f\"{completers['baseline_alcohol_days'].mean():.1f}\", \n",
    "                         f\"{dropouts['baseline_alcohol_days'].mean():.1f}\",\n",
    "                         f\"{dropouts['baseline_alcohol_days'].mean() - completers['baseline_alcohol_days'].mean():.1f}\"],\n",
    "    'Mean Motivation': [f\"{completers['baseline_motivation'].mean():.1f}\", \n",
    "                       f\"{dropouts['baseline_motivation'].mean():.1f}\",\n",
    "                       f\"{dropouts['baseline_motivation'].mean() - completers['baseline_motivation'].mean():.1f}\"]\n",
    "})\n",
    "print(summary_table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b53e92",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Diagnose the Missingness Mechanism  \n",
    "*Analyze the pattern and determine the mechanism.*\n",
    "\n",
    "- **Observed pattern:** Participants with highest baseline alcohol use and lowest motivation are missing follow-up data\n",
    "- **Likely mechanism (MCAR / MAR / MNAR):**  \n",
    "- **Justification:** Why does this pattern suggest your chosen mechanism?  \n",
    "- **Key consideration:** Does the missingness depend on observed baseline characteristics, unobserved follow-up values, or random factors?\n",
    "\n",
    "**Think carefully:** Are high-risk participants missing because:\n",
    "- **(MAR)** They're harder to reach due to their baseline characteristics (lifestyle, contact info changes)?\n",
    "- **(MNAR)** They may have relapsed and don't want to report their current alcohol use levels?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e22fda2",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Choose & Apply a Remedy  \n",
    "Select **one** approach for handling the missing follow-up data:\n",
    "\n",
    "| Option | Approach | When It Makes Sense |\n",
    "|--------|----------|-------------------|\n",
    "| **A. Listwise Deletion** | Remove incomplete participants | Assumes missing data won't bias results |\n",
    "| **B. LOCF (Last Observation Carried Forward)** | Use baseline values for missing follow-up | Assumes no change over time |\n",
    "| **C. Mean Imputation by Timepoint** | Use average follow-up values | Assumes missing participants are \"typical\" |\n",
    "\n",
    "> **Task:** Choose the approach that best fits your diagnosed mechanism and evaluate its assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f35316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Remedy code cell ---\n",
    "# ‚¨áÔ∏è Uncomment the approach you decide to test.\n",
    "\n",
    "# ----- Option A: Listwise Deletion -----\n",
    "# clean = case3.dropna()\n",
    "\n",
    "# ----- Option B: LOCF (Last Observation Carried Forward) -----\n",
    "# clean = (\n",
    "#     case3\n",
    "#     .sort_values([\"participant_id\", \"timepoint\"])\n",
    "#     .groupby(\"participant_id\")\n",
    "#     .ffill()  # forward-fill within participant\n",
    "# )\n",
    "\n",
    "# ----- Option C: Mean Imputation by Timepoint -----\n",
    "# clean = case3.copy()\n",
    "# followup_means = case3[case3[\"timepoint\"] == \"6-month\"].mean(numeric_only=True)\n",
    "# mask = (clean[\"timepoint\"] == \"6-month\")\n",
    "# clean.loc[mask, \"alcohol_days\"] = clean.loc[mask, \"alcohol_days\"].fillna(followup_means[\"alcohol_days\"])\n",
    "# clean.loc[mask, \"motivation\"] = clean.loc[mask, \"motivation\"].fillna(followup_means[\"motivation\"])\n",
    "\n",
    "print(\"Data after chosen remedy:\")\n",
    "display(clean)\n",
    "\n",
    "# Compare trajectories\n",
    "print(f\"\\nMean alcohol days by timepoint:\")\n",
    "print(\"BEFORE imputation (complete cases only):\")\n",
    "print(case3.groupby(\"timepoint\")[\"alcohol_days\"].mean())\n",
    "print(\"\\nAFTER chosen remedy:\")\n",
    "print(clean.groupby(\"timepoint\")[\"alcohol_days\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80c9e9",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Reflection ‚Äì Evaluate Your Remedy  \n",
    "#### **Answer Reflection Questions in Working Table 1 in the Group Document**\n",
    "\n",
    "**Analyze the consequences of your chosen approach:**\n",
    "\n",
    "- **Method used:** [A/B/C]  \n",
    "- **Assumption made:** What does this method assume about the missing data?  \n",
    "- **Bias check:** How did the remedy change the trajectory from baseline to follow-up?  \n",
    "- **Validity concern:** Does your method's assumption match your diagnosed mechanism?  \n",
    "\n",
    "Write 3-4 sentences evaluating whether your chosen remedy creates systematic bias and how it might affect conclusions about treatment effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a26476",
   "metadata": {},
   "source": [
    "## Case 4 ‚Äì \"The Mental Health Screening Study\"  \n",
    "A university counseling center conducts an annual mental health survey asking students about **sleep patterns**, **academic stress**, **social support**, and **depression symptoms**. The final section asks: **\"In the past 2 weeks, how often have you felt down, depressed, or hopeless?\"** (0=Not at all, 1=Several days, 2=More than half the days, 3=Nearly every day)\n",
    "\n",
    "**The Missing Data Problem:** Several students with **high stress levels**, **poor sleep**, and **low social support** left the depression questions blank. However, the research team has strong predictors available: students' **stress scores**, **sleep quality**, and **social connectedness** - all factors known to strongly correlate with depression symptoms.\n",
    "\n",
    "**Your Investigation:**\n",
    "1. **Explore** the pattern of missing data vs. predictor variables\n",
    "2. **Diagnose** the likely missingness mechanism (MCAR / MAR / MNAR)  \n",
    "3. **Choose the best remedy** given the available predictors and mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a2410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 4: Mental health survey with strong predictors\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "case4 = pd.DataFrame({\n",
    "    \"student_id\":      [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
    "    \"stress_level\":    [ 2,   8,   9,   3,   7,   4,   9,   5,   8,   6],  # 1-10 scale\n",
    "    \"sleep_quality\":   [ 8,   3,   2,   7,   4,   6,   2,   7,   3,   5],  # 1-10 scale (higher=better)\n",
    "    \"social_support\":  [ 8,   2,   1,   7,   3,   6,   1,   6,   2,   4],  # 1-10 scale (higher=better)\n",
    "    \"semester_gpa\":    [3.6, 2.1, 1.8, 3.4, 2.5, 3.2, 1.9, 3.0, 2.2, 2.8], # current semester GPA\n",
    "    \n",
    "    # Depression screening question: Missing for high-risk students\n",
    "    \"depression_score\": [ 0,   2, np.nan, 0, np.nan, 1, np.nan, 0, np.nan, 1]  # 0-3 scale, NaN=refused\n",
    "})\n",
    "\n",
    "display(case4)\n",
    "\n",
    "print(\"\\nMissing data pattern:\")\n",
    "print(f\"Missing responses: {case4['depression_score'].isna().sum()} out of {len(case4)} students\")\n",
    "\n",
    "print(\"\\nWho refused to answer:\")\n",
    "missing_students = case4[case4['depression_score'].isna()]\n",
    "print(missing_students[['student_id', 'stress_level', 'sleep_quality', 'social_support']])\n",
    "\n",
    "print(\"\\nPredictor patterns:\")\n",
    "complete_cases = case4.dropna()\n",
    "missing_cases = case4[case4['depression_score'].isna()]\n",
    "\n",
    "print(f\"Mean stress - Complete answers: {complete_cases['stress_level'].mean():.1f}\")\n",
    "print(f\"Mean stress - Missing answers: {missing_cases['stress_level'].mean():.1f}\")\n",
    "print(f\"Mean sleep quality - Complete answers: {complete_cases['sleep_quality'].mean():.1f}\")\n",
    "print(f\"Mean sleep quality - Missing answers: {missing_cases['sleep_quality'].mean():.1f}\")\n",
    "print(f\"Mean social support - Complete answers: {complete_cases['social_support'].mean():.1f}\")\n",
    "print(f\"Mean social support - Missing answers: {missing_cases['social_support'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d146f8",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Diagnose the Missingness Mechanism  \n",
    "*Analyze the pattern and determine the mechanism.*\n",
    "\n",
    "- **Observed pattern:** Students with highest stress, poorest sleep, and lowest social support refused to answer\n",
    "- **Likely mechanism (MCAR / MAR / MNAR):**  \n",
    "- **Justification:** Does the missingness depend on observed characteristics, unobserved values, or random factors?\n",
    "- **Key insight:** What makes this different from purely random missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ffde71",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Choose & Apply a Remedy  \n",
    "Given the available **strong predictors** (stress, sleep, social support), select the best approach:\n",
    "\n",
    "| Option | Approach | When It Makes Sense |\n",
    "|--------|----------|-------------------|\n",
    "| **A. Listwise Deletion** | Remove students with missing data | Assumes missing data won't bias results |\n",
    "| **B. Mean Imputation** | Use average depression score for missing | Assumes missing students are \"typical\" |\n",
    "| **C. Regression Imputation** ‚òÖ | Predict using stress, sleep, social factors | **Uses available predictor information** |\n",
    "\n",
    "> **Task:** Choose the method that best leverages the strong predictors available in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb61977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Remedy code cell ---\n",
    "# ‚¨áÔ∏è Uncomment the approach you decide to test.\n",
    "\n",
    "# ----- Option A: Listwise Deletion -----\n",
    "# clean = case4.dropna()\n",
    "# print(\"Data after listwise deletion:\")\n",
    "# print(f\"Sample size: {len(clean)} students\")\n",
    "# print(f\"Mean depression score: {clean['depression_score'].mean():.2f}\")\n",
    "\n",
    "# ----- Option B: Mean Imputation -----\n",
    "# clean = case4.copy()\n",
    "# depression_mean = case4['depression_score'].mean()  # Calculate from non-missing\n",
    "# clean['depression_score'] = clean['depression_score'].fillna(depression_mean)\n",
    "# print(\"Data after mean imputation:\")\n",
    "# print(f\"Imputed value: {depression_mean:.2f}\")\n",
    "# print(f\"Overall depression mean: {clean['depression_score'].mean():.2f}\")\n",
    "\n",
    "# ----- Option C: Regression Imputation -----\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "clean = case4.copy()\n",
    "complete = case4.dropna()\n",
    "\n",
    "# Fit model on complete cases\n",
    "predictors = ['stress_level', 'sleep_quality', 'social_support', 'semester_gpa']\n",
    "X_train = complete[predictors]\n",
    "y_train = complete['depression_score']\n",
    "\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Predict for missing cases\n",
    "missing_idx = case4['depression_score'].isna()\n",
    "X_missing = case4.loc[missing_idx, predictors]\n",
    "predictions = model.predict(X_missing)\n",
    "\n",
    "# Apply predictions (round to nearest valid score: 0, 1, 2, 3)\n",
    "predictions_rounded = np.clip(np.round(predictions), 0, 3)\n",
    "clean.loc[missing_idx, 'depression_score'] = predictions_rounded\n",
    "clean.loc[missing_idx, 'imputed'] = True\n",
    "clean['imputed'] = clean['imputed'].fillna(False)\n",
    "\n",
    "print(\"Data after regression imputation:\")\n",
    "print(f\"Predicted depression scores for missing students: {predictions_rounded}\")\n",
    "print(f\"Model R¬≤: {model.score(X_train, y_train):.3f}\")\n",
    "\n",
    "display(clean)\n",
    "\n",
    "# Compare approaches\n",
    "print(f\"\\nDepression score comparison:\")\n",
    "print(f\"Complete cases only: {case4['depression_score'].mean():.2f}\")\n",
    "print(f\"After chosen remedy: {clean['depression_score'].mean():.2f}\")\n",
    "\n",
    "# Show model coefficients\n",
    "print(f\"\\nModel insights:\")\n",
    "for pred, coef in zip(predictors, model.coef_):\n",
    "    print(f\"{pred}: {coef:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7df4cf0",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Reflection ‚Äì Evaluate Your Remedy  \n",
    "#### **Answer Reflection Questions in Working Table 1 in the Group Document**\n",
    "\n",
    "**Analyze the consequences of your chosen approach:**\n",
    "\n",
    "- **Method used:** [A/B/C]  \n",
    "- **Predictor utilization:** How well did your method use the available stress/sleep/social support information?\n",
    "- **Bias check:** How did your remedy change the overall depression score estimate?  \n",
    "- **Validity concern:** Does your method's assumption match your diagnosed mechanism?\n",
    "\n",
    "Write 3-4 sentences evaluating: Why is regression imputation particularly well-suited for this mental health scenario? What are the risks of ignoring the strong predictor relationships available in counseling center data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a6bd9",
   "metadata": {},
   "source": [
    "## Exercise 3 ‚Äì Part 2  ¬∑  Missing Data in ABCD Substance Use Environments & Attitudes Surveys ( Case Studies 5 ‚Äì 8 )\n",
    "\n",
    "You and your partners will tackle another four mini-investigations, each built around a different ABCD survey instrument measuring substance use environments and attitudes and a distinctive missing-data wrinkle:\n",
    "\n",
    "| Case | Theme & Instrument | Missing-Data Focus |\n",
    "|------|--------------------|--------------------|\n",
    "| **5** | *Youth Cigarette Expectancies* (`su_y_cigexp`) | Wave-specific ‚Äúnot-administered‚Äù blanks + 777 Declines |\n",
    "| **6** | *Parental Rules* for alcohol & vaping (`su_p_rule`) | Wave design blanks ‚Ä¢ Branch-skip blanks ‚Ä¢ 777 Declines |\n",
    "| **7** | *Household MJ-Smoke Exposure* (`su_p_des__mj__smoke`) | Numeric follow-ups with separate **DK flags 999** |\n",
    "| **8** | *School Attendance & Grades* (`fc_p_sag`) | Ordinal codes 444 / 777 vs. branch logic & IEP skip |\n",
    "---\n",
    "### Team Assignment Table\n",
    "\n",
    "| Group Member | Case Study |\n",
    "|--------------|-----------------|\n",
    "| **Member 1** | Case 5 ‚Äì |\n",
    "| **Member 2** | Case 6 ‚Äì |\n",
    "| **Member 3** | Case 7 ‚Äì |\n",
    "| **Member 4** | Case 8 ‚Äì |\n",
    "\n",
    "### What you‚Äôll do üîç\n",
    "Each notebook follows an identical roadmap:\n",
    "\n",
    "1. **Setup** ‚Äì import libraries & load the supplied *synthetic* CSV.  \n",
    "2. **Codebook Audit** ‚Äì enumerate unique codes and fill in a *missing-count* challenge.  \n",
    "3. **Visual Diagnostics** ‚Äì plot a `missingno` matrix and inspect crosstabs to separate design skips, branch skips, DKs, and true gaps.  \n",
    "4. **Na√Øve vs Clean Metric** ‚Äì compute a quick statistic two ways to show bias.  \n",
    "5. **Reflection Slides** ‚Äì answer three short prompts in your group PowerPoint.\n",
    "\n",
    "Along the way you‚Äôll see **_fill-in-the-blank lines_** (marked `___` or `# TODO`).  \n",
    "Use the Copilot/ChatGPT hints under each challenge if you get stuck.\n",
    "\n",
    "---\n",
    "\n",
    "### Deliverables üìë\n",
    "After you complete your case study, then complete **Working Table 2** in your shared PowerPoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0954a3d1",
   "metadata": {},
   "source": [
    "## Cast Study 5\n",
    "\n",
    "In **Case Study 5** we will explore youth **Cigarette Expectancies (ASCQ)** variables from the ABCD Study.\n",
    "\n",
    "Dataset highlights  \n",
    "* **Instrument**: `su_y_cigexp` ‚Äì thoughts, feelings, and beliefs about smoking  \n",
    "* **Key item**: `su_y_cigexp_001`  ‚ÄúDuring the day, smoking can help kill time if there is nothing to do.‚Äù  \n",
    "  * Added part-way through 2-year follow-up (Wave 2) ‚ñ∫ not administered at Wave 1  \n",
    "* Response codes (ordinal): `1 Never ‚Ä¶ 5 Always`, `0 Uncertain`, plus special `777 Decline`, blanks = missing  \n",
    "\n",
    "In this case study you will  \n",
    "1. Audit the coding structure for the ASCQ items  \n",
    "2. Visualise and diagnose wave-specific missingness  \n",
    "3. Compare na√Øve vs. cleaned means for a positive-expectancy score  \n",
    "4. Reflect on coding, mechanism, and cleaning choices  \n",
    "\n",
    "üìö Helpful references:  \n",
    "* [ABCD Docs ‚Äì su_y_cigexp](https://docs.abcdstudy.org/latest/documentation/non_imaging/su.html#su_y_cigexp)  \n",
    "* [ABCD Data Exploration Portal](https://abcd.deapscience.com/#/my-datasets/create-dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c054a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 Code Cell: imports and data load\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, missingno as msno\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.read_csv(\"synthetic_su_y_cigexp_sample.csv\")  # 20 √ó 2 = 40 rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584f143e",
   "metadata": {},
   "source": [
    "### Step 2: List every unique code‚Äîincluding `0/1‚Äì5`, `777`, blanks‚Äîfor six ASCQ items.\n",
    "\n",
    "**Target variables**\n",
    "\n",
    "* `su_y_cigexp_001`          (new positive item)  \n",
    "* `su_y_cigexp__neg_001`    ‚ÄúSmoking will make a person cough.‚Äù  \n",
    "* `su_y_cigexp__neg_002`    ‚ÄúSmoking makes people look ridiculous or silly.‚Äù  \n",
    "* `su_y_cigexp__pos_001`    ‚ÄúCigarettes help with concentration.‚Äù  \n",
    "* `su_y_cigexp__pos_002`    ‚ÄúWhen someone is sad, smoking helps them feel better.‚Äù  \n",
    "* `su_y_cigexp__pos_003`    ‚ÄúThe look and feel of a cigarette in the mouth is good.‚Äù  \n",
    "\n",
    "---\n",
    "\n",
    "#### Code Challenge #1 ‚Äì Count Missing Values  \n",
    "Fill in the blank so `missing_counts` shows the **number of NaN** entries per column.\n",
    "\n",
    "**Copilot Prompts for Understanding**\n",
    "\n",
    "**1. How can you create a boolean mask to identify missing (NaN) values in a pandas DataFrame or Series?**\n",
    "- Use the `.isna()` method to generate a boolean mask where `True` indicates missing values.\n",
    "\n",
    "**2. How can you summarise the number of `True` values in a boolean mask?**\n",
    "- Apply the `.sum()` method to count the `True` values in the mask.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d8512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 Code Cell\n",
    "vars_to_audit = [\n",
    "    \"su_y_cigexp_001\",\n",
    "    \"su_y_cigexp__neg_001\",\n",
    "    \"su_y_cigexp__neg_002\",\n",
    "    \"su_y_cigexp__pos_001\",\n",
    "    \"su_y_cigexp__pos_002\",\n",
    "    \"su_y_cigexp__pos_003\",\n",
    "]\n",
    "\n",
    "# --- Code Challenge answer placeholder\n",
    "missing_counts = df[vars_to_audit]. _____ # Fill in the missing code here\n",
    "print(missing_counts)\n",
    "\n",
    "# Build tidy code-frequency table\n",
    "records = []\n",
    "for var in vars_to_audit:\n",
    "    vc = df[var].value_counts(dropna=False).sort_index()\n",
    "    for code, cnt in vc.items():\n",
    "        records.append({\"variable\": var, \"code\": code, \"count\": int(cnt)})\n",
    "\n",
    "codebook_df = pd.DataFrame(records)\n",
    "codebook_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9df7fc",
   "metadata": {},
   "source": [
    "### Step 3: Plot a heat-map to spot:  \n",
    "* *Design* missingness ‚Äì Wave 1 blanks for `su_y_cigexp_001`  \n",
    "* *Item* non-response ‚Äì 777 Declines or Uncertain 0s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fd0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = vars_to_audit\n",
    "msno.matrix(df[cols], figsize=(9, 4))\n",
    "plt.title(\"Missingness Matrix ‚Äì ASCQ Items\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d828e33",
   "metadata": {},
   "source": [
    "Verify that **all** Wave 1 rows are missing for `su_y_cigexp_001`, while Wave 2 rows contain real data or 777/0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f604a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df[\"session_id\"], df[\"su_y_cigexp_001\"].isna(),\n",
    "            rownames=[\"Wave\"], colnames=[\"Missing?\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20859bb2",
   "metadata": {},
   "source": [
    "### Step 4 ¬∑ Na√Øve vs. Clean Mean Bias (LO #3)\n",
    "\n",
    "**Prompt:** Compare the mean of a simple *Positive Expectancy Index* = mean of  \n",
    "`su_y_cigexp_001`, `__pos_001`, `__pos_002`, `__pos_003`  \n",
    "\n",
    "* **Na√Øve** analyst: converts all blanks / 777 to 0 (\"Never\").  \n",
    "* **Clean** analyst: treats 777 & blanks as `NaN` and excludes them from the mean.\n",
    "\n",
    "**Goal:** Demonstrate how improper handling of missing data codes can bias substantive findings by comparing two approaches to computing a cigarette expectancy score.\n",
    "\n",
    "**Steps:**\n",
    "1. Create a \"na√Øve\" index by replacing all missing values and 777 codes with 0\n",
    "2. Create a \"clean\" index by properly converting 777 codes to `NaN` and excluding them\n",
    "3. Compare the means and interpret the bias direction and magnitude\n",
    "4. Reflect on implications for research conclusions about adolescent smoking attitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbffc0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_items = [\"su_y_cigexp_001\", \"su_y_cigexp__pos_001\",\n",
    "             \"su_y_cigexp__pos_002\", \"su_y_cigexp__pos_003\"]\n",
    "\n",
    "# Clean copy\n",
    "df_clean = df.copy()\n",
    "df_clean[pos_items] = df_clean[pos_items].replace({777: np.nan})\n",
    "\n",
    "# Na√Øve index (zeros for everything missing/777)\n",
    "naive_index = df[pos_items].replace({np.nan: 0, 777: 0}).astype(float).mean(axis=1)\n",
    "naive_mean  = naive_index.mean()\n",
    "\n",
    "# Clean index (exclude NaN)\n",
    "clean_index = df_clean[pos_items].astype(float).mean(axis=1, skipna=True)\n",
    "clean_mean  = clean_index.mean()\n",
    "\n",
    "pct_diff = (naive_mean - clean_mean) / clean_mean * 100\n",
    "print(f\"Na√Øve mean = {naive_mean:.2f}\")\n",
    "print(f\"Clean mean = {clean_mean:.2f}\")\n",
    "print(f\"Percent difference = {pct_diff:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ccef5d",
   "metadata": {},
   "source": [
    "### Reflection Questions  \n",
    "**Answer Reflection Questions in Working Table 2 in the Group Document**\n",
    "\n",
    "Write about 3 sentences per question.\n",
    "\n",
    "**Q1 (LO 1)** ‚Äì For one ASCQ item, decode blank / 0 / 777 and state which indicates non-response vs. true uncertainty.  \n",
    "\n",
    "**Q2 (LO 2)** ‚Äì Compare Wave 1 ‚Äúdesign‚Äù missingness (item not administered) to 777 Declines. Which mechanism fits each? Explain.  \n",
    "\n",
    "**Q3 (LO 3)** ‚Äì Using Step 4, report na√Øve vs. clean means & sample sizes. Why does zero-filling bias the index, and how does proper cleaning improve it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343068e",
   "metadata": {},
   "source": [
    "## Case Study 6\n",
    "\n",
    "**Case Study 6** explores how ABCD encodes **parental rules** for alcohol and e-cigarette/vape use.\n",
    "\n",
    "Key design facts  \n",
    "* **Instrument:** `su_p_rule` ‚Äì parental approval / household rules.  \n",
    "* **Gate (rules) items:**  \n",
    "  * `su_p_rule__alc_001` ‚Äì rules about adolescent **alcohol** drinking (asked at every wave)  \n",
    "  * `su_p_rule__nic__vape_001` ‚Äì rules about adolescent **ENDS/vaping** (added at the 3-year visit ‚áí *not administered* at Wave-1)  \n",
    "* **Follow-ups:**  \n",
    "  * Alcohol: `__alc_001__01` (same rules for all?) & `__alc_001__02` (penalties?)  \n",
    "  * ENDS:   `__nic__vape_001__01` (same rules for all?)  \n",
    "* Codes: ordinal levels (1‚Äì6), plus `777  Decline`, blanks = missing.\n",
    "\n",
    "What you‚Äôll do  \n",
    "1. Audit code frequencies & missing codes  \n",
    "2. Visualise wave- and branch-related missingness  \n",
    "3. Check branch logic for follow-ups  \n",
    "4. Quantify bias from na√Øve vs. cleaned strict-rule indexing  \n",
    "5. Reflect on 777 vs. branch skips vs. not-administered blanks\n",
    "\n",
    "üìö Docs: [**ABCD Docs: su_p_rule**](https://docs.abcdstudy.org/latest/documentation/non_imaging/su.html#su_p_rule)   |   [Data Explorer](https://abcd.deapscience.com/#/my-datasets/create-dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 Code Cell: imports & load\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, missingno as msno\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.read_csv(\"synthetic_su_p_rule_sample.csv\")   # 40 rows (20√ó2)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ded028",
   "metadata": {},
   "source": [
    "**Goal:** List every unique code‚Äîincluding `777` and blanks‚Äîfor the six variables below.\n",
    "\n",
    "* `su_p_rule__alc_001`\n",
    "* `su_p_rule__alc_001__01`\n",
    "* `su_p_rule__alc_001__02`\n",
    "* `su_p_rule__nic__vape_001`\n",
    "* `su_p_rule__nic__vape_001__01`\n",
    "\n",
    "---\n",
    "\n",
    "### Code Challenge: Count Missing Values  \n",
    "Fill in the blank so `missing_counts` tallies `NaN`s per variable.\n",
    "\n",
    "**Copilot Prompts for Understanding:**\n",
    "\n",
    "1. **How do you identify missing values in a pandas DataFrame?**  \n",
    "   - Use the `.isna()` method to create a boolean mask where `True` indicates missing values.\n",
    "\n",
    "2. **How can you count missing values for each column in a DataFrame?**  \n",
    "   - Chain `.sum()` after `.isna()` to count `True` values (missing entries) for each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2359ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 Code Cell\n",
    "vars_to_audit = [\n",
    "    \"su_p_rule__alc_001\",\n",
    "    \"su_p_rule__alc_001__01\",\n",
    "    \"su_p_rule__alc_001__02\",\n",
    "    \"su_p_rule__nic__vape_001\",\n",
    "    \"su_p_rule__nic__vape_001__01\",\n",
    "]\n",
    "\n",
    "# Challenge answer placeholder\n",
    "missing_counts = df[vars_to_audit]._______ # Fill in\n",
    "print(missing_counts)\n",
    "\n",
    "# tidy codebook table\n",
    "records = []\n",
    "for v in vars_to_audit:\n",
    "    for code, n in df[v].value_counts(dropna=False).sort_index().items():\n",
    "        records.append({\"variable\": v, \"code\": code, \"count\": int(n)})\n",
    "codebook_df = pd.DataFrame(records)\n",
    "codebook_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75dd2bd",
   "metadata": {},
   "source": [
    "### Step 2: Heatmap of Missing Values\n",
    "We expect three distinct missingness sources:\n",
    "\n",
    "1. **Wave design:** ENDS items blank at Wave-1  \n",
    "2. **Branch logic:** follow-ups blank when gate rule is missing or `777`  \n",
    "3. **True non-response:** `777  Decline`\n",
    "\n",
    "Visualise them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef893975",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df[vars_to_audit + [\"session_id\"]], figsize=(9,4))\n",
    "plt.title(\"Missingness ‚Äì Parental Rules Items\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d25ee",
   "metadata": {},
   "source": [
    "### Step 3 ¬∑ Branch-Logic Crosstabs \n",
    "\n",
    "Branch-logic crosstabs help verify whether follow-up questions are properly skipped when gate conditions aren't met, revealing the difference between intentional design skips and true non-response.\n",
    "\n",
    "**Code Challenge #2 ‚Äì Fill-in Crosstab**  \n",
    "Complete the blank so each table shows counts of missing vs. present follow-ups across gate codes.\n",
    "\n",
    "**Copilot Prompts for Understanding:**\n",
    "\n",
    "1. **How do you create a boolean mask to identify missing (NaN) values in a pandas Series?**\n",
    "   - Use the `.isna()` method to create a boolean mask where `True` indicates missing values.\n",
    "\n",
    "2. **What does `pd.crosstab()` show when you cross-tabulate a gate variable with missing status?**\n",
    "   - It reveals the relationship between gate responses and whether follow-up questions are missing, helping verify branch logic patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ca589",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(\"su_p_rule__alc_001\",        \"su_p_rule__alc_001__01\"),\n",
    "         (\"su_p_rule__alc_001\",        \"su_p_rule__alc_001__02\"),\n",
    "         (\"su_p_rule__nic__vape_001\",  \"su_p_rule__nic__vape_001__01\")]\n",
    "\n",
    "for gate, follow in pairs:\n",
    "    ctab = pd.crosstab(df[gate],\n",
    "                       df[follow].____,       # Fill in\n",
    "                       rownames=[gate], colnames=[f\"{follow} missing?\"])\n",
    "    display(ctab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf14427",
   "metadata": {},
   "source": [
    "## Step 4 ¬∑ Na√Øve vs. Clean ‚ÄúStrict-Rule‚Äù Index (LO #3)\n",
    "\n",
    "Create a binary **Strict-Rule** variable:\n",
    "\n",
    "* Alcohol strict = 1 if `su_p_rule__alc_001 == 1` (no drinking ever) else 0  \n",
    "* Na√Øve analyst: treats blanks & 777 as *non-strict* (0)  \n",
    "* Clean analyst: sets blanks/777 to `NaN` and drops them\n",
    "\n",
    "Compare prevalence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_alc = (df[\"su_p_rule__alc_001\"] == 1).astype(float)   # NaN where not 1/else?\n",
    "strict_alc_naive = strict_alc.fillna(0)\n",
    "naive_prop  = strict_alc_naive.mean()\n",
    "\n",
    "strict_alc_clean = strict_alc.copy()\n",
    "strict_alc_clean[df[\"su_p_rule__alc_001\"].isin([777]) | df[\"su_p_rule__alc_001\"].isna()] = np.nan\n",
    "clean_prop  = strict_alc_clean.mean()\n",
    "\n",
    "pct_bias = (naive_prop - clean_prop) / clean_prop * 100\n",
    "print(f\"Na√Øve strict-rule prevalence = {naive_prop:.2%}\")\n",
    "print(f\"Clean strict-rule prevalence = {clean_prop:.2%}\")\n",
    "print(f\"Bias = {pct_bias:+.1f}%  (positive = overestimate)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4158b21",
   "metadata": {},
   "source": [
    "### Reflection Questions  \n",
    "**Answer Reflection Questions in Working Table 2 in the Group Document**\n",
    "\n",
    "Write about 3 sentences per question.\n",
    "\n",
    "**Q1 (LO 1):** For a selected variable, explain what each code represents‚Äîblanks for branch‚Äêskip, 777 for non‚Äêresponse, and levels 1‚Äì6 for valid answers.\n",
    "\n",
    "**Q2 (LO 2)** ‚Äì Using heat-map & crosstabs, contrast wave-design blanks (ENDS Wave-1) with branch blanks (follow-ups) and 777. Assign MCAR / MAR / MNAR.\n",
    "\n",
    "**Q3 (LO 3)** ‚Äì Report na√Øve vs. clean strict-rule prevalence & bias. Explain why zero-filling mis-represents household strictness and how excluding 777/branch blanks fixes it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2962f7",
   "metadata": {},
   "source": [
    "# Case Study 7\n",
    "### Case Study 7 examines parent-reported *marijuana exposure* in the home.\n",
    "\n",
    "Key facts  \n",
    "* **Gate item** `su_p_des__mj__smoke_001`  \n",
    "  * 0 = No, 1 = Yes, 777 = Refused  \n",
    "* **Numeric follow-ups** (asked only if gate == 1)  \n",
    "  * `‚Ä¶__01`  Days per week smoke occurs  (0‚Äì7)  \n",
    "  * `‚Ä¶__02`  Hours per day exposed      (1‚Äì12)  \n",
    "* **DK flags** `‚Ä¶__01__dk`, `‚Ä¶__02__dk` (999 = Don‚Äôt-know)  \n",
    "\n",
    "> **Heads-up:**  (`555 not-administered`, `888 branch-skip`, `999 don‚Äôt-know`, ‚Ä¶) are **defined only for *categorical* ABCD variables**.  \n",
    "> The two follow-up items in this case are *quantitative* (counts of days & hours).  \n",
    "> That means the REDCap form stores **blank cells (`NaN`)** instead of sentinel codes, and relies on a *separate DK-flag column* to capture a ‚ÄúDon‚Äôt-know‚Äù response.\n",
    "\n",
    "In this case you will  \n",
    "1. Audit codes for gate & flag variables  \n",
    "2. Parse blanks vs. DK vs. true missing in numeric columns  \n",
    "3. Visualise patterns of design skips, DK, and unexpected gaps  \n",
    "4. Compare na√Øve vs. cleaned mean exposure hours  \n",
    "5. Reflect on how numeric follow-ups differ from coded categorical items\n",
    "\n",
    "üìö **Helpful references**:\n",
    "- ABCD documentation for this measure: [ABCD Docs: su_p_des](https://docs.abcdstudy.org/latest/documentation/non_imaging/su.html#su_p_des)  \n",
    "- To look up actual variables and definitions, use the [ABCD Data Exploration Portal](https://abcd.deapscience.com/#/my-datasets/create-dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688aabf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 Code Cell ‚Äì imports & data load\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, missingno as msno\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.read_csv(\"synthetic_mj_smoke_sample.csv\")   # 20 √ó 2 = 40 rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4941bb",
   "metadata": {},
   "source": [
    "### Step 1 ¬∑ Codebook Audit\n",
    "**Goal:** List every unique code across the five columns.\n",
    "\n",
    "* `su_p_des__mj__smoke_001`\n",
    "* `su_p_des__mj__smoke_001__01`\n",
    "* `su_p_des__mj__smoke_001__01__dk`\n",
    "* `su_p_des__mj__smoke_001__02`\n",
    "* `su_p_des__mj__smoke_001__02__dk`\n",
    "\n",
    "---\n",
    "\n",
    "### Code Challenge: Count Missing versus DK  \n",
    "Fill in the blank so `missing_and_dk` shows, for each **numeric follow-up** variable, how many rows are *blank* (`NaN`) **and** how many are flagged DK (`==999`) separately.\n",
    "\n",
    "**Copilot Prompts for Understanding:**\n",
    "\n",
    "1. **How do you identify missing values in a pandas DataFrame?**\n",
    "   - Use the `.isna()` method to create a boolean mask where `True` indicates missing values.\n",
    "\n",
    "2. **How can you count missing values for each column in a DataFrame?**\n",
    "   - Chain `.sum()` after `.isna()` to count `True` values (missing entries) for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 Code Cell\n",
    "vars_gate   = [\"su_p_des__mj__smoke_001\"]\n",
    "vars_num    = [\"su_p_des__mj__smoke_001__01\", \"su_p_des__mj__smoke_001__02\"]\n",
    "vars_dk     = [c + \"__dk\" for c in vars_num]\n",
    "\n",
    "# quick counts\n",
    "missing_and_dk = (df[______.to_frame(\"blank\") # Fill in\n",
    "                  .join((df[vars_dk] == 999).sum().rename(\"dk\")))\n",
    "print(missing_and_dk, \"\\n\")\n",
    "\n",
    "# tidy frequency table for gate\n",
    "print(df[\"su_p_des__mj__smoke_001\"].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9def8",
   "metadata": {},
   "source": [
    "### Step 2 ¬∑ Plot the five variables\n",
    "\n",
    "Visualize: \n",
    "* Wave-1 design blanks (numeric follow-ups not asked when gate == 0/NaN)  \n",
    "* DK blanks (value missing but DK flag == 999)  \n",
    "* Any odd gaps that need investigation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c1ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df[vars_gate + vars_num + vars_dk], figsize=(9,4))\n",
    "plt.title(\"Missingness ‚Äì Marijuana-Smoke Exposure items\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0186b73",
   "metadata": {},
   "source": [
    "### Step 3 ¬∑ Branch-Logic Verification\n",
    "We expect numeric follow-ups to be **present** only when gate == 1. Run the code below to ensure this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80b5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in vars_num:\n",
    "    print(f\"\\n{num}\")\n",
    "    display(pd.crosstab(df[\"su_p_des__mj__smoke_001\"],\n",
    "                        df[num].notna(),\n",
    "                        rownames=[\"Gate\"], colnames=[\"present?\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dad7693",
   "metadata": {},
   "source": [
    "### Optional Step 3b ¬∑ Merge Numeric + DK Flag and Visualize  \n",
    "Combine each numeric follow-up with its DK flag so you can treat ‚ÄúDK‚Äù as an explicit category when plotting.\n",
    "\n",
    "**New variable**  \n",
    "`hours_exposure_combined` =  \n",
    "\n",
    "| If | Then |  \n",
    "|---|---|  \n",
    "| `‚Ä¶__02__dk == 999` | string `\"DK\"` |  \n",
    "| numeric value present | that value (int) |  \n",
    "| else | `NaN` (branch skip / not asked) |\n",
    "\n",
    "After creating the combined column, make a histogram (numeric bars) plus a single bar for `\"DK\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3b57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Hours + DK flag into one column\n",
    "hours_num = df[\"su_p_des__mj__smoke_001__02\"]\n",
    "hours_dk  = df[\"su_p_des__mj__smoke_001__02__dk\"]\n",
    "\n",
    "hours_combined = hours_num.copy()\n",
    "hours_combined[hours_dk == 999] = \"DK\"   # string label\n",
    "df[\"hours_exposure_combined\"] = hours_combined\n",
    "\n",
    "# Quick visualization: numeric bars + DK bar\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "# separate numeric and DK\n",
    "numeric_vals = pd.to_numeric(hours_combined, errors=\"coerce\").dropna()\n",
    "numeric_vals.hist(ax=ax, bins=range(1,14), edgecolor=\"black\")\n",
    "# add DK count\n",
    "dk_count = (hours_combined == \"DK\").sum()\n",
    "ax.bar(x=13, height=dk_count, width=0.8, color=\"gray\", label=\"DK\")\n",
    "ax.set_xticks(list(range(1,13)) + [13])\n",
    "ax.set_xticklabels(list(range(1,13)) + [\"DK\"])\n",
    "ax.set_xlabel(\"Hours per day (or DK)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Exposure Hours (numeric) vs. DK responses\")\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60fde9b",
   "metadata": {},
   "source": [
    "### Step 4 ¬∑ Na√Øve vs Clean Mean Exposure Hours\n",
    "Create **HoursExposure** = numeric column `‚Ä¶__02`.\n",
    "\n",
    "* **Na√Øve** analyst ‚Üí sets blanks/DK‚Üí0 hours  \n",
    "* **Clean** analyst ‚Üí blanks where gate !=1 or DK flag ==999 are `NaN`\n",
    "\n",
    "Compare means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d9531",
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_raw   = df[\"su_p_des__mj__smoke_001__02\"].copy()\n",
    "\n",
    "# make a cleaned version\n",
    "hours_clean = hours_raw.copy()\n",
    "# remove DKs\n",
    "hours_clean[df[\"su_p_des__mj__smoke_001__02__dk\"] == 999] = np.nan\n",
    "# remove branch skips (gate != 1)\n",
    "hours_clean[df[\"su_p_des__mj__smoke_001\"] != 1] = np.nan\n",
    "\n",
    "naive_mean = hours_raw.fillna(0).mean()\n",
    "clean_mean = hours_clean.mean()\n",
    "bias       = (naive_mean - clean_mean) / clean_mean * 100\n",
    "\n",
    "print(f\"Na√Øve mean = {naive_mean:.2f} hrs\")\n",
    "print(f\"Clean mean = {clean_mean:.2f} hrs\")\n",
    "print(f\"Bias = {bias:+.1f}%  (pos = over-estimate)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86153a6",
   "metadata": {},
   "source": [
    "### Reflection Questions  \n",
    "**Answer Reflection Questions in Working Table 2 in the Group Document**\n",
    "\n",
    "Write about 3 sentences per question.\n",
    "\n",
    "**Q1 (LO 1)** ‚Äì Decode blank, DK flag 999, and numeric values 0-12.  \n",
    "Which blanks are design skips, DK, or data glitches?\n",
    "\n",
    "**Q2 (LO 2)** ‚Äì Using the heat-map & crosstabs, identify MCAR / MAR / MNAR patterns across the five variables.\n",
    "\n",
    "**Q3 (LO 3)** ‚Äì Summarise na√Øve vs. clean mean HoursExposure and bias. Explain why filling blanks with 0 distorts average exposure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa1b1a2",
   "metadata": {},
   "source": [
    "## Case Study 8\n",
    "\n",
    "In **Case Study 8**, we will work with a synthetic version of the **School Attendance & Grades [Parent]** dataset (`fc_p_sag`) for 14-year-old participants.  \n",
    "This dataset replicates how the ABCD Study structures and codes data for parent-reported school experiences, including:\n",
    "\n",
    "- **Ordinal grade categories** (coded 1‚Äì12)\n",
    "- **Special values** like `444 = Not Applicable`, `777 = Decline to answer`, and blank cells for skipped questions\n",
    "- **Branch logic**, where follow-up questions are only asked if a parent first reports their child has an IEP (Individualized Education Plan)\n",
    "\n",
    "In this case study, you will:\n",
    "1. Audit the coding structure for key variables  \n",
    "2. Visualize and diagnose missingness patterns  \n",
    "3. Apply branch logic checks and develop a cleaning strategy\n",
    "\n",
    "---\n",
    "\n",
    "üìö **Helpful references**:\n",
    "- ABCD documentation for this measure: [ABCD Docs ‚Äì fc_p_sag](https://docs.abcdstudy.org/latest/documentation/non_imaging/fc.html#fc_p_sag)  \n",
    "- To look up actual variables and definitions, use the [ABCD Data Exploration Portal](https://abcd.deapscience.com/#/my-datasets/create-dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 Code Cell: imports and data load\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the synthetic dataset for Case Study 4\n",
    "df = pd.read_csv(\"synthetic_fc_p_sag_sample.csv\")\n",
    "\n",
    "# Preview the first 5 rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ea4c3",
   "metadata": {},
   "source": [
    "### Step 1 ¬∑ Codebook Audit (LO #1)\n",
    "\n",
    "**Goal:** Enumerate and inspect **all unique codes**‚Äîincluding `0/1`, `444` (Not Applicable), `777` (Decline), `999` (Don‚Äôt Know), and blanks‚Äîfor six key variables. This will help you map each code back to ABCD‚Äôs standardized conventions and prepare a tidy lookup table.\n",
    "\n",
    "**Target variables:**\n",
    "- `fc_p_sag_004`  \n",
    "- `fc_p_sag_005`  \n",
    "- `fc_p_sag_006a`  \n",
    "- `fc_p_sag_006a__01`  \n",
    "- `fc_p_sag_006a__02___2`  \n",
    "\n",
    "Run `value_counts(dropna=False)` on each to capture blanks as well as special codes.\n",
    "\n",
    "### Code Challenge: Counting Missing Values\n",
    "\n",
    "Fill in the blank so that `missing_counts` becomes a pandas `Series` showing the number of `NaN` (missing) entries for each audited variable:\n",
    "\n",
    "#### Copilot Prompt Suggestions\n",
    "\n",
    "**1. Which pandas DataFrame method helps you identify missing (NaN) values across columns?**\n",
    "- Use the `.isna()` method to create a boolean mask where `True` indicates missing values.\n",
    "\n",
    "**2. How can you aggregate a boolean mask of missing entries to count them per column?**\n",
    "- Apply `.sum()` on the boolean mask to count `True` values (missing entries) for\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd6453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 Code Cell: enumerate codes and counts for each target variable\n",
    "\n",
    "# List of variables to audit\n",
    "vars_to_audit = [\n",
    "    \"fc_p_sag_004\",\n",
    "    \"fc_p_sag_005\",\n",
    "    \"fc_p_sag_006a\",\n",
    "    \"fc_p_sag_006a__01\",\n",
    "    \"fc_p_sag_006a__02___2\"\n",
    "]\n",
    "\n",
    "# Code Challenge: Display missing counts for each variable\n",
    "missing_counts = df[vars_to_audit].___________\n",
    "print(missing_counts)\n",
    "\n",
    "# Prepare an empty list to collect codebook entries\n",
    "records = []\n",
    "\n",
    "# Loop through each variable, get counts (including NaN), and record them\n",
    "for var in vars_to_audit:\n",
    "    vc = df[var].value_counts(dropna=False).sort_index()\n",
    "    total = len(df)\n",
    "    for code, cnt in vc.items():\n",
    "        records.append({\n",
    "            \"variable\": var,\n",
    "            \"code\":      code,\n",
    "            \"count\":     int(cnt),\n",
    "            \"percent\":   cnt / total * 100\n",
    "        })\n",
    "\n",
    "# Build a tidy summary DataFrame\n",
    "codebook_df = pd.DataFrame(records)\n",
    "\n",
    "# Display for review\n",
    "codebook_df.sort_values([\"variable\", \"code\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eb86c9",
   "metadata": {},
   "source": [
    "### Step 3 ¬∑ Missingness Heat-Map \n",
    "Visualizing missing data patterns helps you quickly identify where and how data are absent‚Äîrevealing branch-logic skips, item nonresponse, and potential systematic gaps. We‚Äôll use the **missingno** library to generate a heat‚Äêmap matrix showing present vs. missing values for our key GPA and IEP variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 Code Cell: plot missingness matrix\n",
    "\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the four variables of interest\n",
    "cols = [\n",
    "    \"fc_p_sag_004\",\n",
    "    \"fc_p_sag_005\",\n",
    "    \"fc_p_sag_006a\",\n",
    "    \"fc_p_sag_006a__01\",\n",
    "    \"fc_p_sag_006a__02___2\"\n",
    "]\n",
    "\n",
    "# Plot the missingness matrix\n",
    "msno.matrix(df[cols], figsize=(8, 4), fontsize=12)\n",
    "plt.title(\"Missingness Heat-Map for fc_p_sag Variables\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4d21e9",
   "metadata": {},
   "source": [
    "### Step 4 ¬∑ Branch-Logic Crosstab\n",
    "\n",
    "When a parent reports **no IEP** (`0`) or **Don‚Äôt Know** (`999`), the follow-up items (`fc_p_sag_006a__01`, `fc_p_sag_006a__02___2`) should be blank (missing).  \n",
    "The crosstab below shows, for each IEP response, how many follow-up entries are missing (`True`) vs. present (`False`), confirming the branch logic.\n",
    "\n",
    "### Code Challenge: Branch-Logic Crosstab Fill-in-the-Blank\n",
    "\n",
    "Complete the blank so that `ctab` cross-tabulates **IEP status** against whether each follow-up column is missing:\n",
    "\n",
    "**Copilot Prompt Suggestions**\n",
    "\n",
    "1. Which pandas Series method gives you a boolean mask of missing (NaN) values?\n",
    "- Use the `.isna()` method to identify missing values in a pandas Series.\n",
    "\n",
    "2. How does `pd.crosstab` treat True/False when summarizing two series?\n",
    "- `pd.crosstab` treats `True` and `False` as distinct categories, allowing you to summarize their counts across two\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 Code Cell: verify branch-logic skip patterns\n",
    "\n",
    "followups = [\"fc_p_sag_006a__01\", \"fc_p_sag_006a__02___2\"]\n",
    "\n",
    "for col in followups:\n",
    "    ctab = pd.crosstab(\n",
    "        df[\"fc_p_sag_006a\"],\n",
    "        df[col].___, # Fill in the blank\n",
    "        rownames=[\"IEP Status\"],\n",
    "        colnames=[f\"{col} Missing?\"]\n",
    "    )\n",
    "    print(f\"\\nMissingness crosstab for `{col}`:\")\n",
    "    display(ctab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7128cbca",
   "metadata": {},
   "source": [
    "### Step 5 ¬∑ Na√Øve vs. Clean Mean Bias Demonstration\n",
    "\n",
    "**Prompt:** Simulate how a na√Øve analyst who replaces **all** missing and special codes (`NaN`, `444`, `777`) with `0` in `fc_p_sag_004` (unexcused absences) would bias the mean estimate.  \n",
    "\n",
    "1. Compute the ‚Äúna√Øve‚Äù mean after zero‚Äêreplacement.  \n",
    "2. Retrieve the properly cleaned mean from `df_clean`.  \n",
    "3. Calculate the percent difference.  \n",
    "4. Comment briefly on whether the na√Øve approach under- or over-estimates the true mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 Code Cell: compare na√Øve vs. cleaned mean for fc_p_sag_004\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# First, create a properly cleaned dataset\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Proper cleaning: convert special codes to NaN, keep only valid numeric responses\n",
    "df_clean[\"fc_p_sag_004\"] = df_clean[\"fc_p_sag_004\"].replace({444: np.nan, 777: np.nan})\n",
    "\n",
    "# 1. Na√Øve mean: replace NaN, 444, 777 ‚Üí 0\n",
    "naive_series = df[\"fc_p_sag_004\"].replace({np.nan: 0, 444: 0, 777: 0}).astype(float)\n",
    "naive_mean   = naive_series.mean()\n",
    "\n",
    "# 2. Clean mean: properly handle special codes by excluding them\n",
    "clean_mean   = df_clean[\"fc_p_sag_004\"].mean()  # This automatically excludes NaN values\n",
    "\n",
    "# 3. Percent difference\n",
    "pct_diff     = (naive_mean - clean_mean) / clean_mean * 100\n",
    "\n",
    "# 4. Print results\n",
    "print(f\"Na√Øve mean = {naive_mean:.2f}\")\n",
    "print(f\"Clean mean = {clean_mean:.2f}\")\n",
    "print(f\"Percent difference = {pct_diff:.1f}%\")\n",
    "\n",
    "# 5. Show sample sizes for context\n",
    "print(f\"\\nSample sizes:\")\n",
    "print(f\"Na√Øve approach (includes all data): {len(naive_series)} participants\")\n",
    "print(f\"Clean approach (excludes special codes): {df_clean['fc_p_sag_004'].count()} participants\")\n",
    "\n",
    "# 6. Interpretation\n",
    "if naive_mean < clean_mean:\n",
    "    print(f\"\\nüìâ The na√Øve approach UNDERESTIMATES the true mean by {abs(pct_diff):.1f}%\")\n",
    "else:\n",
    "    print(f\"\\nüìà The na√Øve approach OVERESTIMATES the true mean by {pct_diff:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2564e5dd",
   "metadata": {},
   "source": [
    "### Reflection Questions  \n",
    "**Answer Reflection Questions in Working Table 2 in the Group Document**\n",
    "\n",
    "Write about 3 sentences per question.\n",
    "\n",
    "**Q1 (LO 1)** ‚Äì For one variable, decode blank / 444 / 777 / 999 and state which indicates logical skip vs. true non-response.\n",
    "\n",
    "**Q2 (LO 2)** ‚Äì Using heat-map & crosstabs, contrast structured codes (444/777/999) with NaN blanks. Assign MCAR / MAR / MNAR mechanisms and explain why.\n",
    "\n",
    "**Q3 (LO 3)** ‚Äì Report na√Øve vs. clean means & sample sizes for `fc_p_sag_004`. Explain why zero-filling biases the mean and how proper cleaning fixes it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
